---
image: "https://lh3.googleusercontent.com/ku3r-ISJBv11QaqCMydLkGH2hOhCDjAeXhQ0dSEqae1U1Eg3N6ksg8MtQoanF2rIawGMnURLLzgMIstvrDqQn1fLas8KoQ_Ru3L5M8UzeNY=w1200-h630-n-nu"
imageType: source
status: draft
category: Safety
source: Google DeepMind
sourceUrl: https://deepmind.google/blog/strengthening-our-frontier-safety-framework/
publishedAt: 2025-10-23T23:44:10.000Z
---

---
title: "Strengthening our Frontier Safety Framework"
category: "Safety"
source: "Google DeepMind"
source_url: "https://deepmind.google/blog/strengthening-our-frontier-safety-framework/"
published_at: "2025-10-23T23:44:10.000Z"
status: "draft"
---

# Strengthening our Frontier Safety Framework

<!-- HERO_IMAGE: add an editorial image URL or local path here -->

## WHAT HAPPENED
September 22, 2025 Responsibility & Safety Strengthening our Frontier Safety Framework We’re expanding our risk domains and refining our risk assessment process.AI breakthroughs are transforming our everyday lives, from advancing mathematics, biology and astronomy to realizing the potential of personalized education. As we build increasingly powerful AI models, we’re committed to responsibly developing our technologies and taking an evidence-based approach to staying ahead of emerging risks.Today, we’re publishing the third iteration of our Frontier Safety Framework (FSF) — our most comprehensive approach yet to identifying and mitigating severe risks from advanced AI models.This update builds upon our ongoing collaborations with experts across industry, academia and government. We’ve also incorporated lessons learned from implementing previous versions and evolving best practices in frontier AI safety.Key updates to the FrameworkAddressing the risks of harmful manipulationWith this update, we’re introducing a Critical Capability Level (CCL)* focused on harmful manipulation — specifically, AI models with powerful manipulative capabilities that could be misused to systematically and substantially change beliefs and behaviors in identified high stakes contexts over the course of interactions with the model, reasonably resulting in additional expected harm at severe scale.This addition builds on and operationalizes research we’ve done to identify and evaluate mechanisms that drive manipulation from generative AI.

## WHY IT MATTERS
- _Draft placeholder_
- _Draft placeholder_
- _Draft placeholder_

## WHAT THIS ENABLES
- _Draft placeholder_
- _Draft placeholder_

## SOURCE
https://deepmind.google/blog/strengthening-our-frontier-safety-framework/

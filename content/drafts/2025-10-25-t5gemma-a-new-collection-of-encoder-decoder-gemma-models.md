---
image: "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/HeroBlog-meta.2e16d0ba.fill-1200x600.png"
imageType: source
status: draft
category: Industries
source: Google DeepMind
sourceUrl: https://deepmind.google/blog/t5gemma-a-new-collection-of-encoder-decoder-gemma-models/
publishedAt: 2025-10-25T18:14:00.000Z
---

---
title: "T5Gemma: A new collection of encoder-decoder Gemma models"
category: "Industries"
source: "Google DeepMind"
source_url: "https://deepmind.google/blog/t5gemma-a-new-collection-of-encoder-decoder-gemma-models/"
published_at: "2025-10-25T18:14:00.000Z"
status: "draft"
---

# T5Gemma: A new collection of encoder-decoder Gemma models

<!-- HERO_IMAGE: add an editorial image URL or local path here -->

## WHAT HAPPENED
JULY 9, 2025 In the rapidly evolving landscape of large language models (LLMs), the spotlight has largely focused on the decoder-only architecture. While these models have shown impressive capabilities across a wide range of generation tasks, the classic encoder-decoder architecture, such as T5 (The Text-to-Text Transfer Transformer), remains a popular choice for many real-world applications. Encoder-decoder models often excel at summarization, translation, QA, and more due to their high inference efficiency, design flexibility, and richer encoder representation for understanding input.

## WHY IT MATTERS
- _Draft placeholder_
- _Draft placeholder_
- _Draft placeholder_

## WHAT THIS ENABLES
- _Draft placeholder_
- _Draft placeholder_

## SOURCE
https://deepmind.google/blog/t5gemma-a-new-collection-of-encoder-decoder-gemma-models/

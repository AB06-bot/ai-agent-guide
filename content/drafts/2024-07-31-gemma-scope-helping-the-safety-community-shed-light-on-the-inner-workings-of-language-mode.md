---
image: "https://lh3.googleusercontent.com/JSywa5d9GFAiu836G_LW_fziUgLVKbglK70xFfkLWSg64NVM6sjQbcocum1kSlTGyJYmmI3DLwMIbllQbSCvTMlIBPCRGRWJHN9yV9njGpY=w1200-h630-n-nu"
imageType: source
status: draft
category: Safety
source: Google DeepMind
sourceUrl: https://deepmind.google/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/
publishedAt: 2024-07-31T15:59:19.000Z
---

---
title: "Gemma Scope: helping the safety community shed light on the inner workings of language models"
category: "Safety"
source: "Google DeepMind"
source_url: "https://deepmind.google/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/"
published_at: "2024-07-31T15:59:19.000Z"
status: "draft"
---

# Gemma Scope: helping the safety community shed light on the inner workings of language models

<!-- HERO_IMAGE: add an editorial image URL or local path here -->

## WHAT HAPPENED
July 31, 2024 Models Announcing a comprehensive, open suite of sparse autoencoders for language model interpretability.To create an artificial intelligence (AI) language model, researchers build a system that learns from vast amounts of data without human guidance. As a result, the inner workings of language models are often a mystery, even to the researchers who train them. Mechanistic interpretability is a research field focused on deciphering these inner workings.

## WHY IT MATTERS
- _Draft placeholder_
- _Draft placeholder_
- _Draft placeholder_

## WHAT THIS ENABLES
- _Draft placeholder_
- _Draft placeholder_

## SOURCE
https://deepmind.google/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/

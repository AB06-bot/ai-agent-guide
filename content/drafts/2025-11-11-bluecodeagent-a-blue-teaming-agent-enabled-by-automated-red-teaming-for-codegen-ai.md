---
image: "https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/BlueCodeAgent-TWLIFB-1200x627-1.jpg"
imageType: source
status: draft
category: Safety
source: Microsoft Research
sourceUrl: https://www.microsoft.com/en-us/research/blog/bluecodeagent-a-blue-teaming-agent-enabled-by-automated-red-teaming-for-codegen-ai/
publishedAt: 2025-11-11T17:00:00.000Z
---

---
title: "BlueCodeAgent: A blue teaming agent enabled by automated red teaming for CodeGen AI"
category: "Safety"
source: "Microsoft Research"
source_url: "https://www.microsoft.com/en-us/research/blog/bluecodeagent-a-blue-teaming-agent-enabled-by-automated-red-teaming-for-codegen-ai/"
published_at: "2025-11-11T17:00:00.000Z"
status: "draft"
---

# BlueCodeAgent: A blue teaming agent enabled by automated red teaming for CodeGen AI

<!-- HERO_IMAGE: add an editorial image URL or local path here -->

## WHAT HAPPENED
Introduction Large language models (LLMs) are now widely used for automated code generation across software engineering tasks. However, this powerful capability in code generation also introduces security concerns. Code generation systems could be misused for harmful purposes, such as generating malicious code.

## WHY IT MATTERS
- _Draft placeholder_
- _Draft placeholder_
- _Draft placeholder_

## WHAT THIS ENABLES
- _Draft placeholder_
- _Draft placeholder_

## SOURCE
https://www.microsoft.com/en-us/research/blog/bluecodeagent-a-blue-teaming-agent-enabled-by-automated-red-teaming-for-codegen-ai/
